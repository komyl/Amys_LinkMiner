# Amys_LinkMiner

Amys_LinkMiner is a web scraping tool designed to extract hyperlinks from a given URL. It iteratively collects up to 1000 unique links, ensuring efficiency and robustness in its web crawling. The project is written in Python and leverages popular libraries such as BeautifulSoup and Requests.

# Features

- Extracts hyperlinks from a user-provided URL.

- Collects up to 1000 unique links.

- Handles HTTP errors gracefully and prevents crashes.

- Converts relative URLs to absolute URLs for better reliability.

- Includes rate limiting to avoid overwhelming web servers.

# Requirements

To run Amys_LinkMiner, you need to have the following Python packages installed:

- beautifulsoup4

- requests

- validators

- lxml

You can install these dependencies using the following command:
```
pip install -r requirements.txt
```
# Why We Used Each Library

- beautifulsoup4: Used for parsing HTML content and extracting specific elements, such as hyperlinks (<a> tags). It makes navigating and searching through HTML easier, which is essential for web scraping.

- requests: Used to send HTTP requests to web servers and retrieve the HTML content of web pages. It is a powerful and user-friendly library for handling HTTP operations in Python.

- validators: Used to validate URLs to ensure that they are well-formed and can be requested without causing errors. This helps to filter out invalid or malformed links during the scraping process.

- lxml: Used as a parser with BeautifulSoup for fast and efficient parsing of HTML and XML documents. It enhances the performance of BeautifulSoup when processing large web pages.

# Installation

1- Ensure Python 3.6 or higher is installed on your machine. You can download Python from python.org.

2- Clone the repository:

```
git clone github.com/komyl/Amys_LinkMiner.git
```
3- Navigate to the project directory:

```
cd Amys_LinkMiner
```

4- Install the required dependencies:
```
pip install -r requirements.txt
```

# Usage

1- Run the script:
```
python Amys_LinkMiner.py
```

2- You will be prompted to enter the starting URL for scraping.

# Example

After running the script, enter the desired URL when prompted:

```
Please enter the URL to start scraping: https://example.com
```

The script will then begin to process and display the URLs it finds, up to 1000 unique links.

# Important Notes

- Be cautious when scraping websites. Always review and respect the ```robots.txt``` rules of a website.

- Avoid scraping too frequently or aggressively, as this can lead to your IP being blocked.

- This tool may have limitations when dealing with very large or complex websites, or pages with dynamically loaded content.

- Restricted or login-protected pages may not be scraped successfully due to access permissions.

# License

This project is licensed under the MIT License - see the LICENSE file for details.

## Authors

This project was developed by **Komeyl Kalhorinia** and **Ameneh Zarebidoki**. You can reach us at [Komylfa@gmail.com][AmenehZarebidoki@gmail.com] for any inquiries or contributions.

## Made with ❤️ by Komeyl Kalhorinia and AmenehZarebidoki
